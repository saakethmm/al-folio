---
title: "BELT-2: Bootstrapping EEG-to-Language representation alignment for multi-task brain decoding"
authors: 
year: 2023
published: false
---
2024-02-11, 1120

Status: #paper

Tags: [[BCIs]], [[Conformer]], [[Contrastive Objective]], [[Decoding]], [[EEG]], [[Generative model]], [[Language]], [[Prefix-tuning]], [[Cognitive Neuroscience]], [[Computational Neuroscience]], [[Research]], [[Vision]]

#### What *question* do they seek to answer? 
LLMs have been combined with various modalities including vision and robotics has picked up, though what about using it with decoding human intentions?

**Why?**
* Relatively unexplored area
* Most previous work has notable restrictions on vocab size (when doing direct brain-to-text decoding)
* Prior methods that use supervised language models are limited to the *single-task* setting


#### What's their *idea* to answer that question? 
They propose BELT-2 which has 3 key features:
1. Introduces BPE-level contrastive learning for EEG-to-language alignment
	* Byte-pair encoding (BPE)
2. Introduces prompt-based multi-task encoder for EEG work (Q-Conformer?)
3. Cost-effective approach connecting LLM with EEG encoder


**How do they come up with it?**

*No explanation!*

#### What *data/models* do they use to test it? 

**Data**: EEG data (ZuCo dataset)
* EEG data recorded during natural reading with eye-tracking data for word-level segmentation
**Models**:
* Q-Conformer (EEG Encoder)
	* 1. Discrete conformer $\rightarrow$ 2. Context Transformer (C-Former) $\rightarrow$ 3. Query prompt
		1. **Discrete Conformer**
			* ***Intuition***: Captures primitive patterns from input EEG embeddings
			* EEG mapped into fixed-size EEG embeddings 
			* Conformer (Convolutional Transformer) which maps EEG embeddings $\mathbf{e}$ into continuous tokens $\mathbf{h}$ (==why conformer?==)
			* Convert tokens $\mathbf{h}$ into discrete tokens $\mathbf{b}$ by finding the nearest distance code from a codebook (==why?==)
			* Trained with VQ (Vector Quantizer) loss (1): ![[Screenshot 2024-02-11 at 11.59.46 AM.png]]
				* 1 & 2: Codebook and commitment loss (minimize information loss between EEG signal and tokens)
				* 3: Balancing term to ensure codebook doesn't use only a few tokens
				* 4: Reconstruction loss to get as close as possible to original signal
		2. **C-Former and Query Prompt**
			* ***Intuition***: contains context information specific to query 
			* Input: query prompts & discrete EEG tokens ${\bf h}$ 
			* Via *self-attention*, query prompts interact 
			* Via *cross-attention*, these learned patterns interact with EEG tokens 
				* ==Why not together in cross-attention?==
* EEG to language alignment
	* Combine 2 contrastive objectives and pretraining objective to (1)
		* Byte pair encoding-level contrastive loss (BPE-CL)![[Screenshot 2024-02-11 at 2.31.27 PM.png]]
		* Negative contrastive loss (NCL)![[Screenshot 2024-02-11 at 2.31.32 PM.png]]
	* Used to learn the task-specific query prompt, which is then used as input to C-Former to decode the EEG recording
* Use of *Speculative Augmentation* to provide more prefix-tuning samples for better prompts which are 
	* ***Prefix-tuning***: concatenates trainable tensor on to the end of each input to each transformer block (low-compute fine-tuning)
		* Sort of like learnable prompt engineering
 

**Why?**

**Data**: 
* EEG was the modality under consideration

**Models**: Very unclear why this method was chosen, so many different models combined! 
2. At least for the C-Former, the querying mechanism allows the use of a pretrained backbone (in this case **BART**-large)


#### What *experiments* do they run? Pick main ones.
1. Translation from EEG to language benchmark (ZuCo dataset)
2. Summarization

**Why?**Â 


#### What else would *I* like to see?
* More explanation regarding the model architecture choices (many models presented as if they are common knowledge which I don't believe they are)
* Not much intuition given

  
#### What do their results say (*my* interpretation)? Same ones.
1. Table 1![[Screenshot 2024-02-11 at 2.43.12 PM.png]]
	  * Significant improvement over previous models 
2. Table 3![[Screenshot 2024-02-11 at 2.47.11 PM.png]]
	* Noticeably improved performance when including the pretrained model as backbone for the C-Former


#### What do their results say (*their* interpretation)? Same ones.
1. Not much analysis of the results, just reports the benchmarks
2. Same as above.
	* Poorer performance than language-to-language translation (==but this is not even comparable to the task at hand, right?==)
	* Uses eye-tracking movements to segment, so not necessarily ready for portable use
  

#### What are their conclusions?
1. "Pioneering" contribution bridging LLMs to brain signals


**Any future directions to extend?**
* Reproducing results is a big test, since there are so many components to this model
* Another would be to simplify the design
* Potentially apply to other modalities such as MEG (fMRI already captured by [[@tangSemanticReconstructionContinuous2023]])


