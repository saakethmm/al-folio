---
layout: post
published: false
title:  <em>What are the most important questions?</em>
date:   2024-03-10
description: List of ideas and topics I'm fascinated by and why they're important
tags: ideas
categories: research
comments: false # looks disgusting right now, need to fix this too...
---

<blockquote>
<!--- TODO: insert Richard Hamming quote about important problems--->
<!--- TODO: Seymour Papert is a g -->
<!-- The supreme goal of all theory is to make the irreducible basic elements as simple and as few as
possible without having to surrender the adequate representation of a single datum of experience - Einstein -->
</blockquote>
 
 
<hr>

Inspired by Richard Hamming's famous talk, ["You and Your Research"](http://www.cs.virginia.edu/~robins/YouAndYourResearch.html), I've been thinking about what are the most important questions to ask. The way he phrases the question is also quite unique, in the sense that it's not just about what I think is the most lucrative to solve, more so what I think has the best angle of attack to solve (while also being significant).

Since I'm in the field of AI and neuroscience, I'll focus on those areas. I'll also try to keep this list updated as I learn more and my interests change. 

<!-- I'm on the side of humans, not machines. Let's augment intelligence and help others along the way, not replace us. ahem... cognition labs -->

### Thoughts

1. I think one of the biggest questions is interpretability.
    * Without a deeper understanding of how AI models make decisions, there will never be trust in the public sphere to use them in critical applications, limiting its impact.
    * In neuroscience, understanding the mechanisms behind which behavior is generated is arguably *the* question to answer.
    * Moreover, interpretability tugs at one of the most fundamental questions in the universe: *what is the mechanism behind intelligence*?
    * I find the question of alignment (after going through plenty of forum posts on alignmentforum and lesswrong) a little vague and too subjective. Interpretability adds a more concrete and objective angle to the question of alignment.
    * Perhaps controversially, I also don't think the biggest question is creating AGI (we don't even fully understand what this means even). The biggest question (and more interesting one) is understanding intelligence, after which creating one is just a matter of time? (look into more)
    * More than AGI, I think augmenting intelligence at the intersection of BCI and AI is criminally underexplored, and this will have a larger impact on humans than AGI will.
    * Mech interp on *in-silico* models of the brain can help us understand information flow and processing in the brain?
    * Similarly about mech interp and identifying how language works in the brain/LLMs? How is information actually processed at a mechanistic level in both the brain and LLMs? Can we find any similarities in the *processes* underlying the two?
<!-- combining ideas from interpretability with data from the brain can help us reveal the mechanisms of intelligence, also will test theories of intelligence!-->



2. There is a fascination notion behind the idea that as systems converge in the type of tasks they perform, the *representations* underlying those systems converge as well. This implies that given a particular task, environment, and constraints, intelligent systems naturally evolve to a particular form. (task-optimized models for another post maybe...)

    * Understanding how information is communicated and understood, sort of extending Claude Shannon's ideas one step further. How can one use information (via language) to best learn the outside world?
    * This seems to have massive implications in modern discourse with understanding issues and communicating (e.g., politics & news), education (how to teach better), ...


### Vision
In the future, I can imagine things like:
* Using mind to visualize the app which some agent can then create immediately
* Police officer asking witness to visualize crime scene so that some diffusion model can recreate scene

<hr>




