---
layout: post
published: false
title:  <em>What are the most important questions?</em>
date:   2024-03-10
description: List of ideas and topics I'm fascinated by and why they're important
tags: ideas
categories: research
comments: false # looks disgusting right now, need to fix this too...
---

<blockquote>
<!--- TODO: insert Richard Hamming quote about important problems--->
<!--- TODO: Seymour Papert is a g>
</blockquote>
 
 
<hr>

Inspired by Richard Hamming's famous talk, ["You and Your Research"](http://www.cs.virginia.edu/~robins/YouAndYourResearch.html), I've been thinking about what are the most important questions to ask. The way he phrases the question is also quite unique, in the sense that it's not just about what I think is the most lucrative to solve, more so what I think has the best angle of attack to solve (while also being significant).

Since I'm in the field of AI and neuroscience, I'll focus on those areas. I'll also try to keep this list updated as I learn more and my interests change.

### Thoughts

1. I think the biggest question without a doubt is interpretability.
    * Without a deeper understanding of how AI models make decisions, there will never be trust in the public sphere to use them in critical applications, limiting its impact.
    * In neuroscience, understanding the mechanisms behind which behavior is generated is arguably *the* question to answer.
    * Moreover, interpretability tugs at one of the most fundamental questions in the universe: *what is the mechanism behind intelligence*?
    * I find the question of alignment (after going through plenty of forum posts on alignmentforum and lesswrong) a little vague and too subjective. Interpretability adds a more concrete and objective angle to the question of alignment.
    * Perhaps controversially, I also don't think the biggest question is creating AGI (we don't even fully understand what this means even). The biggest question (and more interesting one) is understanding intelligence, after which creating one is just a matter of time? (look into more)
    * More than AGI, I think augmenting intelligence at the intersection of BCI and AI is criminally underexplored, and this will have a larger impact on humans than AGI will.

2. How can we improve the generalizability of AI models to match the abilities of our brains, so we can use them in real world applications? Our brains seem to be able to learn from far fewer examples than the best AI models (add ref) yet are still able to adapt and generalize to new situations seamlessly. What can we learn to improve this?
3. Similarly about mech interp and identifying how language works in the brain/LLMs? How is information actually processed at a mechanistic level in both the brain and LLMs? Can we find any similarities in the *processed* underlying the two?
4. Understanding how information is communicated and understood, sort of extending Claude Shannon's ideas one step further. How can one use information (via language) to best learn 
the outside world?
* This seems to have massive implications in modern discourse with understanding issues and communicating (e.g., politics & news), education (how to teach better), ...


### Vision
In the future, I can imagine things like:
* Using mind to visualize the app which some agent can then create immediately
* Police officer asking witness to visualize crime scene so that some diffusion model can recreate scene

<hr>




