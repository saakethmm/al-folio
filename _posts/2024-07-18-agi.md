---
layout: distill
published: false
title: Is research even necessary?
date:   2024-07-03
description: AGI is coming... (or not?)
tags: AI, future
categories: reflection, opinion,
comments: true # looks disgusting right now, need to fix this too...

authors:
  - name: Saaketh Medepalli
    affiliations: 
      name: Machine Learning Department, Carnegie Mellon University

bibliography: 2024-06-23-intentions.bib
---

Commentary on Leopold Aschenbrenner's [Situational Awareness blog series](https://situational-awareness.ai/).

*Why does any of this even matter? AGI is coming soon anyways, right? Wouldn't this be able to solve our problem?*

A community in Silicon Valley are noting the possibility of human-level intelligence in the next few years, as we continue to scale foundation models with more data and compute. This is a whole other topic for another time, but to keep it short and sweet I'm quite skeptical of this claim, if not for the sole reason that everyone needs to clarify what precisely is meant by human-level intelligence (all AI people care about is Turing test across multiple domains, ... don't know if I'm on board with this). One of the best ways to do that is to more closely inspect the principles underlying intelligence, which is a natural result of representational alignment.

But let's suppose all of the AI doomers are correct and we have "superintelligent" AI (ASI) within the next five years or so (insert citation regarding this claim). *Even if* this is the case, humans aren't going anywhere.

Given the right data (e.g., fMRI/MEG), it does seem plausible that an advanced enough AI will be able to read from literature and propose new experiments and do those experiments which can then advance cognitive neuroscience (or other fields). In turn, *even if* an AI is still able to 

... Once we reach AGI, how do we find meaning in this world?

Fundamentally, I'm bullish on nature, not on machines. There's something missing still to understand humans. 

- sakana.ai and future of research