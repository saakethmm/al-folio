---
layout: distill
published: false
title:  Thoughts on AI (& Human) Alignment
date:   2024-01-23
description: Things I like and dislike, what I'm looking forward to
tags:  LLMs, personalization
categories: essays
comments: true # looks disgusting right now, need to fix this too...

authors:
  - name: Saaketh Medepalli
    affiliations: 
      name: Machine Learning Department, Carnegie Mellon University
---

TODO: think about the message I'm trying to send (what's the takeaway message of the story) - something along the lines of a rethinking of incentives towards meaning and improving well-being, rather than market forces being the end-all/be-all (back this claim up with recent examples), intro motivates with article that has made these observations 
... but the title is about AI alignment -> need to spend more time on this topic and the claims of the field, etc....

<!-- Intro -->

I recently came across [Ted Chiang's article on Superintelligent AI](https://www.buzzfeednews.com/article/tedchiang/the-real-danger-to-civilization-isnt-ai-its-runaway) on the topic of AI alignment, which raises some very interesting points:

* doomsday vision of AI running amok because of the specification problem (i.e., paperclip maximizer) - funny how such an AI lacks any such insight (as one might expect an AGI-artificial general intelligence-to have) of whether what it's doing is for the good of the public (I don't see much evidence for why LLMs couldn't support this behavior)
* creating bogeyman out of AI alignment problem when the alignment problem has always existed - just with the corporations themselves, let loose by capitalism

You might be shocked to learn (at least I was) that this article was written almost 8 years ago, back in 2017, yet I find that the article's main points largely remain the same.

Yoshua Bengio also writes about this similar issue, where individual entity goals conflict with the collective interests, writing about the idea that corporations are also misaligned entities. 

<!-- relevance to present day, recent events -->
To those who might be questioning the claims of the articles, one only need to look at the slew of recent events that have been either directly or indirectly caused by some of the perverse side effects of tech corporations:

In the light of several recent events which I came across, the tragedy of [Sewell Setzer on character.ai](https://www.theguardian.com/technology/2024/oct/23/character-ai-chatbot-sewell-setzer-death) to the ([Mormon mom story](https://www.biography.com/crime/a60319774/ruby-franke-story) - problem of family vlogging...)

This is just a tiny glimpse into the devastating effects caused by setting the objective for tech companies to optimize for clicks/views. This is not to say that it's all doom and gloom - of course there are services provided by companies like Google, Meta, and X that provide utility in people's lives. Tools like Google Search, WhatsApp, and academic accounts on X have been particularly useful. All I mean to say is that the current objectives certainly are not *necessary* to achieve these services - there could have been more beneficial ways to go about these platforms that would have provided inherent value to people's lives.

Indeed, perhaps there might have been a way to develop such products without the devastating effects of social media (Haidt) or the hyperpolarization that has pervaded our culture 

<!-- The misalignment  -->
I can't help but notice that the mainstream work in AI safety doesn't *really* talk about these things (misses the mark?). It's more sexy to talk about catastrophic risks and other scientific problems, not to mention it provides *way* more funding for academics (reference, as a student in the CMU MLD, it's funny how so many professors are concerned about this problem compared to 3-5 years ago). Relatively mundane-sounding concerns like robustness and generalization of models morphed into the cool (TODO: come up with something?). 

It was about a couple months ago that I came across work from the Meaning Alignment Institute, and relatedly, the Center for Humane Technology. Both try to get deeper at what's going on

* go a little deeper into what they are trying to say
* recent readings on Humane tech (maybe post screenshot from the slide)
* trying to get at what the real goal of tech should be - capitalize on people's desires or help them *thrive* (TODO: provide definition from Humane Tech)

<!-- connection to the broader picture -->

What folks here aptly point out is that the mechanisms for gathering information on what users like are flawed, and can have horrible consequences

revealed preferences are one thing

* another beautiful point made by Chiang in the article challenges the assumption that demand in the market parallels what people *actually* want
In other words, are company incentives centered around increasing market share at all costs *actually aligned* with human sources of meaning? What is the ultimate goal here? 

Of course, it is tempting to scoff at the seeming alternative (i.e., communism), suggesting that at least it reflects human interests, rather than being something prescribed by an ideology.

- what I think is that sometimes what people think benefits them isn't what actually brings them meaning (that's not to say someone else knows either, it's the tool that understands what constitutes meaning for humans) - brings to ancient notions of wisdom in the East (the distinction between *manas* and *buddhi*, where one hinges on emotion, the other on reason)

- TODO: explore any relevant connections to human decision-making/bounded rationality/prospect theory? But ultimately this should be grounded in human well-being and intentional action


# References

https://yoshuabengio.org/2023/05/22/how-rogue-ais-may-arise/
https://yoshuabengio.org/2023/05/07/ai-scientists-safe-and-useful-ai/
https://thegradient.pub/we-need-positive-visions-for-ai-grounded-in-wellbeing/

