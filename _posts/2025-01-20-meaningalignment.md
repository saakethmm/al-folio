---
layout: distill
published: false
title:  Thoughts on AI (& Human) Alignment
date:   2024-01-23
description: Things I like and dislike, what I'm looking forward to
tags:  LLMs, personalization
categories: essays
comments: true # looks disgusting right now, need to fix this too...

authors:
  - name: Saaketh Medepalli
    affiliations: 
      name: Machine Learning Department, Carnegie Mellon University
---


<!-- Intro -->

I recently came across [Ted Chiang's article on Superintelligent AI](https://www.buzzfeednews.com/article/tedchiang/the-real-danger-to-civilization-isnt-ai-its-runaway) on the topic of AI alignment, which raises some very interesting points:

* doomsday vision of AI running amok because of the specification problem (i.e., paperclip maximizer) - funny how such an AI lacks any such awareness (as one might expect an AGI to have) of whether it aligns with human values (I don't see much evidence for why LLMs couldn't support this behavior)
* creating bogeyman out of AI alignment problem when the alignment problem has always existed - just with the tech companies themselves 

You might be shocked to learn, as I was, that this article was written almost 8 years ago, back in 2017, yet such conversations have hardly changed.

<!-- relevance to present day, recent events -->
To those who might be questioning the claims of the articles, one only need to look at the slew of recent events that have been either directly or indirectly caused by some of the perverse side effects of tech corporations:

* backlash over TikTok ban (reversed due to outcry of Gen Z, but where is the justification for such outcry? addiction?)
* The tragedy of [Sewell Setzer on character.ai](https://www.theguardian.com/technology/2024/oct/23/character-ai-chatbot-sewell-setzer-death)

In the light of several recent events which I came across, including the TikTok ban (& reversal <link>), the tragedy of character.ai, to Shari Franke story ([Mormon mom story](https://www.biography.com/crime/a60319774/ruby-franke-story) - is that really an example of perverse incentives driving it though?)


* People recognize the devastating effects of social media (Jonathan Haidt)

<!-- something seems off -->
I can't help but notice that the mainstream work in AI safety doesn't *really* talk about these things (misses the mark?). It's more sexy to talk about catastrophic risks and other scientific problems, not to mention it provides *way* more funding for academics (reference, as a student in the CMU MLD, it's funny how so many professors suddenly care about this problem).

It was about a couple months ago that I came across work from the Meaning Alignment Institute, and relatedly, the Center for Humane Technology. Both try to get deeper at what's going on

* go a little deeper into what they are trying to say
* recent readings on Humane tech (maybe post screenshot from the slide)

<!-- connection to the broader picture -->

What folks here aptly point out is that the mechanisms for gathering information on what users like are flawed, and can have horrible consequences

revealed preferences are one thing
- what I think is that sometimes what people think benefits them isn't what actually brings them meaning (that's not to say someone else knows either, it's the tool that understands what constitutes meaning for humans)


# References

